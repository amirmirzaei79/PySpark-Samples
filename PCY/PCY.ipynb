{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b4b2f28-f457-4a28-8630-5d1358ac653a",
   "metadata": {},
   "source": [
    "# PCY Algorithm (Using PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1aecf-1a4c-4755-8668-d44f5a2e29a8",
   "metadata": {},
   "source": [
    "## Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a942b3-146e-49c3-b762-c356007671f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb67922-a3ed-4c13-9be3-ea41c0359b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b180d5b-9552-4062-af10-f2c10318d087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b8de9-9262-4e47-8c09-45fc62ad945f",
   "metadata": {},
   "source": [
    "## Initializing Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77eb6aa5-a5d4-4dbd-8dfc-874ecbe2ccf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 16:46:07 WARN Utils: Your hostname, amir-GL553VE resolves to a loopback address: 127.0.1.1; using 192.168.43.187 instead (on interface wlp2s0)\n",
      "23/04/22 16:46:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 16:46:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.43.187:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1da01a0580>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f032dc60-12a5-40be-8d86-c123eb02d23b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30161bc6-09de-47a8-bce5-706f36d9430c",
   "metadata": {},
   "source": [
    "## Reading Input and Necessary Transformation\n",
    "* Reading Input\n",
    "* Creating ID for Items\n",
    "* Throwing Away Unnecessary Columns\n",
    "* Changing Itemname for ItemID\n",
    "* Transforming bills to form (BillNo, {set of ItemIDs})\n",
    "* Transforming bills to form (BillNo, \\[list of sorted ItemIDs\\])\n",
    "Sorting is useful for creating and counting combinations later as it makes sure they are always in the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b407b60-32bc-4bbb-bca8-870e2ac47531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BillNo: string (nullable = true)\n",
      " |-- Itemname: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Price: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = spark.read.options(header=True, delimiter=\";\").csv(\"transactions.csv\")\n",
    "transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3dc5981-dd2a-4080-bba4-77724b340ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======>                                                   (1 + 7) / 8]\r"
     ]
    }
   ],
   "source": [
    "items = transactions.select(transactions.Itemname).distinct().rdd.map(lambda x: x.Itemname).zipWithIndex()\n",
    "bills = transactions.select(transactions.BillNo, transactions.Itemname).rdd.map(lambda x: (x.Itemname, x.BillNo)).join(items).values()\n",
    "item_ids = items.map(lambda x: (x[1], x[0]))\n",
    "del items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1965dc42-7efd-4d39-9b3b-3e39f4d5a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa346fb-742d-4370-a485-b34688c89276",
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets = bills.map(lambda x: (x[0], {x[1]})).reduceByKey(lambda x, y: x.union(y)).mapValues(lambda x: sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6be7a08a-0e9d-422f-b54b-730d189674ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Baskets:  21663\n",
      "Number of Unique Items:  4186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "number_of_items = item_ids.count()\n",
    "number_of_baskets = baskets.count()\n",
    "print(\"Number of Baskets: \", number_of_baskets)\n",
    "print(\"Number of Unique Items: \", number_of_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cbd65-9038-4052-97ca-cd76e47157d5",
   "metadata": {},
   "source": [
    "## Setting Up Constants\n",
    "* Support \n",
    "* Number of Buckets\n",
    "* hash function (for hashing pairs into buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb67767-5585-4607-8b0c-10edc17a1e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "support = 250 # Smaller number for support creates too many frequent items and pairs. Larger number for support eliminates frequent sets with 4 items\n",
    "num_buckets = 10000 # Larger numbers don't change number of frequent buckets (limitation of hash function and data) and Smaller number (lower than 8500) will be saturated\n",
    "min_confidence = 0.8 # for rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "125fe5ea-01c8-4b38-804f-d82bd236d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_function = lambda x: (hash(x[0] + x[1]) % num_buckets)\n",
    "# hash_function = lambda x: ((x[0] + x[1]) % num_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980557f-70cf-49f9-bd01-7e0c9f927128",
   "metadata": {},
   "source": [
    "## Approach 1 - Using Bit Vectors (For 1st and 2nd Passes)\n",
    "1. Finding Frequent Items and Frequent Buckets (Count Using Map (flatMap) and Reduce (By Key) and then filtering)\n",
    "2. Creating Bit Vectors for frequent items and buckets in the driver\n",
    "    1. Creating Bit Vectors Using a list of accumulators (list is sent to every executor, accumulators are shared)\n",
    "    2. Moving Bit Vectros to numpy arrays in the driver\n",
    "    3. Sending bit vectors to executors along with functions to find frequent pairs (happens automatically in spark)\n",
    "3. Next passes\n",
    "    1. Creating sets from frequent sets of last step (sets of size n+1 from sets of size n)\n",
    "    2. Creating sets using union, if a set has exactly n+1 items we check if it has n+1 subsets in frequent sets of last step (to minimize candidate sets, all subsets of a frequent set must be frequent as well)\n",
    "    3. Map: if all elements of a candidate sets are present in a basket, we map it to (set, 1)\n",
    "    4. Counting using reduce by key, then filtering\n",
    "    5. Continuing to next pass (repeat this cycle) as long as this pass yields atleast n+1 frequent set\n",
    "    \n",
    "* We Use ItemIDs to reduce data that is being processed\n",
    "* In the beggining an RDD is created to save (id, name) for each item Later another RDD is created to store same information only for frequent_items (using join with frequent items)\n",
    "* In the end we collect (id, name) of frequent items (as map) in driver and replace ids with names in rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c53327f7-444c-4c27-a8dc-6525b5e10e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequents = [] # for storing items, pairs and frequent sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d452b0-f14a-4080-b858-f0606d7c646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Finding frequent items and their support\n",
    "frequent_items = baskets.flatMap(lambda x: [(item, 1) for item in x[1]]).reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > support)\n",
    "frequent_item_ids = frequent_items.join(item_ids).map(lambda x: (x[0], x[1][1]))\n",
    "frequents.append(frequent_items.collect()) # Stroing frequent items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e020f5b-a2c3-47dc-8441-543edbb8e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding frequent buckets\n",
    "frequent_buckets = baskets.flatMap(\n",
    "    lambda x: [(hash_function(comb), 1) for comb in combinations(x[1], 2)]\n",
    ").reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > support).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a017d2f-e6b5-467e-bfea-b821996cf1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Frequent Items:  591\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Frequent Items: \", len(frequents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ef724fb-cd81-49ac-a9ea-78ccdb4742df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Frequent Buckets:  7798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Frequent Buckets: \", frequent_buckets.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5861a129-9067-40d5-b6f5-35af6d7803f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting frequent items to bitvectors\n",
    "\n",
    "num_items = number_of_items\n",
    "bit_vectors_accs = [sc.accumulator(0) for i in range(math.ceil(num_items / 32))] # Creating Accumulators to make parallel operations possible\n",
    "\n",
    "frequent_items.foreach(lambda x: bit_vectors_accs[x[0] // 32].add(1 << (x[0] % 32))) # Generating Bit Vectors\n",
    "\n",
    "# Moving bitvectors to np.array\n",
    "items_bitvector = np.empty(math.ceil(num_items / 32), dtype=np.uint32)\n",
    "for i in range(len(bit_vectors_accs)):\n",
    "    items_bitvector[i] = bit_vectors_accs[i].value\n",
    "\n",
    "# Getting rid of redundant data\n",
    "del bit_vectors_accs, frequent_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17496193-d2dc-4a17-9a7f-1c1212946b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting frequent buckets to bitvectors\n",
    "\n",
    "bit_vectors_accs = [sc.accumulator(0) for i in range(math.ceil(num_buckets / 32))] # Creating Accumulators to make parallel operations possible\n",
    "\n",
    "frequent_buckets.foreach(lambda x: bit_vectors_accs[x // 32].add(1 << (x % 32))) # Generating Bit Vectors\n",
    "\n",
    "# Moving bitvectors to np.array\n",
    "buckets_bitvector = np.empty(math.ceil(num_buckets / 32), dtype=np.uint32)\n",
    "for i in range(len(bit_vectors_accs)):\n",
    "    buckets_bitvector[i] = bit_vectors_accs[i].value\n",
    "\n",
    "# Getting rid of redundant data\n",
    "del bit_vectors_accs, frequent_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6817490c-3790-4776-9b0e-47e073924b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Finding Frequent Pairs\n",
    "\n",
    "def generate_pairs(x):\n",
    "    # Map function\n",
    "    # From all possible pairs for each baskets, only returns those where both items and corresponding bucket are frequent\n",
    "    pairs = []\n",
    "    for comb in combinations(x[1], 2):\n",
    "        comb_hash = hash_function(comb)\n",
    "        if ((items_bitvector[comb[0] // 32] >> (comb[0] % 32)) % 2 == 1) and \\\n",
    "           ((items_bitvector[comb[1] // 32] >> (comb[1] % 32)) % 2 == 1) and \\\n",
    "           ((buckets_bitvector[comb_hash // 32] >> (comb_hash % 32)) % 2 == 1):\n",
    "              pairs.append((comb, 1))\n",
    "    return pairs\n",
    "\n",
    "# Counting occurances of each candidate pair and filtering frequent pairs\n",
    "frequent_pairs = baskets.flatMap(generate_pairs).reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > support)\n",
    "\n",
    "frequents.append(frequent_pairs.collect())\n",
    "del frequent_pairs, items_bitvector, buckets_bitvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "495aa91e-17b3-467c-b826-1feb202cb943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Frequent Pairs:  389\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Frequent Pairs: \", len(frequents[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ee70c9b-ed75-4fc7-a5df-1cfc5a73d6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sets with  3  items:  79\n",
      "Number of sets with  4  items:  3\n"
     ]
    }
   ],
   "source": [
    "# 3rd Pass and forward\n",
    "\n",
    "def generate_function(candidates):\n",
    "    # This function returns map function based on candidates sets given as input\n",
    "    # Created for convinience and cleaning code\n",
    "    def generate_sets(x):\n",
    "        # Map function created based on candidate sets\n",
    "        sets = []\n",
    "        for s in candidates:\n",
    "            # For each candidate set, map it to (set, 1) if all of set's members are present in basket\n",
    "            flag = True\n",
    "            for item in s:\n",
    "                if item not in x[1]:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                sets.append((s, 1))\n",
    "        return sets\n",
    "    return generate_sets\n",
    "\n",
    "\n",
    "n = 2 # current size of largest sets\n",
    "while len(frequents[-1]) > n+1:\n",
    "    # Start of a pass\n",
    "    frequent_sets = [set(x[0]) for x in frequents[-1]] # Converting tuples to set to use union function\n",
    "    candidates_set = set() # Creating a set for storing candidates to avoid duplicates\n",
    "    for i in range(len(frequent_sets)):\n",
    "        for j in range(i + 1, len(frequent_sets)):\n",
    "            # Create union for each pair of frequent sets with size n\n",
    "            s = frozenset(frequent_sets[i].union(frequent_sets[j]))\n",
    "            if len(s) == n + 1:\n",
    "                # if the created set has exactly n+1 items, check if it has n+1 subsets if frequent sets with size n, if so, add it to candidate sets\n",
    "                c = 0\n",
    "                for k in range(len(frequent_sets)):\n",
    "                    c += 1 if frequent_sets[k].issubset(s) else 0\n",
    "                if c == n + 1:\n",
    "                    candidates_set.add(s)\n",
    "    candidates = [tuple(s) for s in candidates_set] # converting candidate sets to tuples (for reducing memory footprint and faster operations)\n",
    "    \n",
    "    # If there is atleast one candidate set, count it's occurances\n",
    "    if len(candidates) > 0:\n",
    "        fs = baskets.flatMap(generate_function(candidates)).reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > support)\n",
    "        frequents.append(fs.collect())\n",
    "        n += 1\n",
    "        print(\"Number of sets with \", n, \" items: \", len(frequents[-1]))\n",
    "    else:\n",
    "        frequents.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3c2d6-31dc-4f52-bf53-0cef746358df",
   "metadata": {},
   "source": [
    "## Finding Association Rules\n",
    "1. Finding all association rules, keeping those with their confidence over a threshold\n",
    "2. Sorting rules based on their interest\n",
    "3. Changin ItemIDs for their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfe56cda-7241-4f53-bfe9-e7e6a67e386f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_subset_of(subset: tuple, superset: tuple):\n",
    "    if type(subset) == int:\n",
    "        subset = (subset,)\n",
    "    subset = sorted(subset)\n",
    "    superset = sorted(superset)\n",
    "    \n",
    "    difference = []\n",
    "    subset_index = 0\n",
    "    superset_index = 0\n",
    "    subset_len = len(subset)\n",
    "    superset_len = len(superset)\n",
    "    while subset_index < subset_len and superset_index < superset_len:\n",
    "        if subset[subset_index] == superset[superset_index]:\n",
    "            subset_index += 1\n",
    "            superset_index += 1\n",
    "        elif subset[subset_index] < superset[superset_index]:\n",
    "            return False, None\n",
    "        else: # subset[subset_index] > superset[superset_index]\n",
    "            while superset_index < superset_len and subset[subset_index] > superset[superset_index]:\n",
    "                difference.append(superset[superset_index])\n",
    "                superset_index += 1\n",
    "    \n",
    "    if subset_index >= subset_len:\n",
    "        difference.extend(superset[superset_index:])\n",
    "        return True, difference\n",
    "    else:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "335cee34-a06d-4fbb-9bcf-f89a88609856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def probability(items, itemID, number_of_baskets):\n",
    "    item_found = False\n",
    "    i = 0\n",
    "    items_len = len(items)\n",
    "    while i < items_len and not item_found:\n",
    "        if itemID == items[i][0]:\n",
    "            item_support = items[i][1]\n",
    "            item_found = True\n",
    "        i += 1\n",
    "    \n",
    "    if item_found:\n",
    "        return item_support / number_of_baskets\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2a262b9-d10e-4cb3-ab23-9dd0f38628d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rules = [] # to save rules in this format ((set, item), confidence, interest)\n",
    "for i in range(len(frequents) - 1):\n",
    "    for base in frequents[i]:\n",
    "        for target in frequents[i + 1]:\n",
    "            is_subset, diff = is_subset_of(base[0], target[0])\n",
    "            if is_subset:\n",
    "                confidence = target[1] / base[1]\n",
    "                if confidence >= min_confidence:\n",
    "                    interest = abs(confidence - probability(frequents[0], diff[0], number_of_baskets))\n",
    "                    rules.append(((base[0], diff[0]), confidence, interest))\n",
    "\n",
    "rules.sort(key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b853eac0-1647-4f58-8a4d-d0c90ba201f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Rules Extracted: (Rules sorted by interest)\n",
      "\n",
      "Rule  1 :  3353 -> 3411  - interrest:  0.89496  - confidence:  0.91135\n",
      "Rule  2 :  (762, 2321) -> 2602  - interrest:  0.86887  - confidence:  0.89273\n",
      "Rule  3 :  (560, 2641, 2403) -> 1934  - interrest:  0.86114  - confidence:  0.90615\n",
      "Rule  4 :  (2403, 2641) -> 1934  - interrest:  0.85850  - confidence:  0.90351\n",
      "Rule  5 :  (560, 2641) -> 1934  - interrest:  0.82526  - confidence:  0.87027\n",
      "Rule  6 :  (560, 2641, 1934) -> 2403  - interrest:  0.82280  - confidence:  0.86957\n",
      "Rule  7 :  (226, 1432) -> 1117  - interrest:  0.81461  - confidence:  0.86207\n",
      "Rule  8 :  3411 -> 3347  - interrest:  0.81460  - confidence:  0.83380\n",
      "Rule  9 :  (1432, 2028, 2334) -> 1117  - interrest:  0.80969  - confidence:  0.85714\n",
      "Rule 10 :  2548 -> 308  - interrest:  0.80844  - confidence:  0.83129\n",
      "Rule 11 :  (1934, 2641) -> 2403  - interrest:  0.80448  - confidence:  0.85124\n",
      "Rule 12 :  2321 -> 2602  - interrest:  0.79042  - confidence:  0.81429\n",
      "Rule 13 :  (560, 2641) -> 2403  - interrest:  0.78837  - confidence:  0.83514\n",
      "Rule 14 :  (226, 2334) -> 1117  - interrest:  0.77908  - confidence:  0.82653\n",
      "Rule 15 :  2641 -> 1934  - interrest:  0.77700  - confidence:  0.82201\n",
      "Rule 16 :  (27, 548, 1045) -> 1585  - interrest:  0.77161  - confidence:  0.86688\n",
      "Rule 17 :  (1432, 2028) -> 1117  - interrest:  0.75837  - confidence:  0.80583\n",
      "Rule 18 :  (1432, 2334) -> 1117  - interrest:  0.75700  - confidence:  0.80446\n",
      "Rule 19 :  (226, 2028) -> 1117  - interrest:  0.75647  - confidence:  0.80392\n",
      "Rule 20 :  (1045, 2976) -> 1585  - interrest:  0.74599  - confidence:  0.84127\n",
      "Rule 21 :  (27, 1045) -> 1585  - interrest:  0.71200  - confidence:  0.80728\n",
      "Rule 22 :  (85, 1045) -> 1585  - interrest:  0.70762  - confidence:  0.80290\n",
      "Rule 23 :  (548, 1045) -> 1585  - interrest:  0.70628  - confidence:  0.80156\n",
      "Rule 24 :  (223, 1045) -> 1585  - interrest:  0.70588  - confidence:  0.80115\n"
     ]
    }
   ],
   "source": [
    "print(len(rules), \"Rules Extracted: (Rules sorted by interest)\")\n",
    "print()\n",
    "index_format = \"%\" + str(math.ceil(math.log10(len(rules)))) + \"d\" # to make sure spacing will be the same for all rules\n",
    "for i in range(len(rules)):\n",
    "    print(\"Rule\", index_format%(i+1), \": \", rules[i][0][0], \"->\", rules[i][0][1], \" - interrest: \", \"%1.5f\"%rules[i][2], \" - confidence: \", \"%1.5f\"%rules[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5d929f7-2ad2-432c-8968-8c9efa18ab2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rule_id_to_name(id_name_map, rule):\n",
    "    rule_base = None\n",
    "    if type(rule[0][0]) == int:\n",
    "        rule_base = id_name_map[rule[0][0]]\n",
    "    else:\n",
    "        rule_base = tuple([id_name_map[item_id] for item_id in rule[0][0]])\n",
    "    \n",
    "    rule_target = id_name_map[rule[0][1]]\n",
    "    \n",
    "    return ((rule_base, rule_target), interest, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c455ff23-8c5c-4124-80b3-366a8ab48a35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Changin IDs in rules with item names\n",
    "id_name = frequent_item_ids.collectAsMap()\n",
    "rules_named = [rule_id_to_name(id_name, rule) for rule in rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad34ffb7-49fe-4f2c-a641-6d60e70de746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Rules Extracted: (Rules sorted by interest)\n",
      "\n",
      "Rule  1 : \n",
      "REGENCY TEA PLATE PINK -> REGENCY TEA PLATE GREEN  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule  2 : \n",
      "('SET/20 RED RETROSPOT PAPER NAPKINS', 'SET/6 RED SPOTTY PAPER CUPS') -> SET/6 RED SPOTTY PAPER PLATES  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule  3 : \n",
      "('REGENCY CAKESTAND 3 TIER', 'PINK REGENCY TEACUP AND SAUCER', 'ROSES REGENCY TEACUP AND SAUCER') -> GREEN REGENCY TEACUP AND SAUCER  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule  4 : \n",
      "('ROSES REGENCY TEACUP AND SAUCER', 'PINK REGENCY TEACUP AND SAUCER') -> GREEN REGENCY TEACUP AND SAUCER  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule  5 : \n",
      "('REGENCY CAKESTAND 3 TIER', 'PINK REGENCY TEACUP AND SAUCER') -> GREEN REGENCY TEACUP AND SAUCER  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule  6 : \n",
      "('REGENCY CAKESTAND 3 TIER', 'PINK REGENCY TEACUP AND SAUCER', 'GREEN REGENCY TEACUP AND SAUCER') -> ROSES REGENCY TEACUP AND SAUCER  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule  7 : \n",
      "('CHARLOTTE BAG PINK POLKADOT', 'STRAWBERRY CHARLOTTE BAG') -> RED RETROSPOT CHARLOTTE BAG  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule  8 : \n",
      "REGENCY TEA PLATE GREEN -> REGENCY TEA PLATE ROSES  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule  9 : \n",
      "('STRAWBERRY CHARLOTTE BAG', 'CHARLOTTE BAG SUKI DESIGN', 'WOODLAND CHARLOTTE BAG') -> RED RETROSPOT CHARLOTTE BAG  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 10 : \n",
      "WOODEN TREE CHRISTMAS SCANDINAVIAN -> WOODEN STAR CHRISTMAS SCANDINAVIAN  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 11 : \n",
      "('GREEN REGENCY TEACUP AND SAUCER', 'PINK REGENCY TEACUP AND SAUCER') -> ROSES REGENCY TEACUP AND SAUCER  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 12 : \n",
      "SET/6 RED SPOTTY PAPER CUPS -> SET/6 RED SPOTTY PAPER PLATES  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 13 : \n",
      "('REGENCY CAKESTAND 3 TIER', 'PINK REGENCY TEACUP AND SAUCER') -> ROSES REGENCY TEACUP AND SAUCER  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 14 : \n",
      "('CHARLOTTE BAG PINK POLKADOT', 'WOODLAND CHARLOTTE BAG') -> RED RETROSPOT CHARLOTTE BAG  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 15 : \n",
      "PINK REGENCY TEACUP AND SAUCER -> GREEN REGENCY TEACUP AND SAUCER  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 16 : \n",
      "('JUMBO SHOPPER VINTAGE RED PAISLEY', 'JUMBO STORAGE BAG SUKI', 'JUMBO BAG PINK POLKADOT') -> JUMBO BAG RED RETROSPOT  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 17 : \n",
      "('STRAWBERRY CHARLOTTE BAG', 'CHARLOTTE BAG SUKI DESIGN') -> RED RETROSPOT CHARLOTTE BAG  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 18 : \n",
      "('STRAWBERRY CHARLOTTE BAG', 'WOODLAND CHARLOTTE BAG') -> RED RETROSPOT CHARLOTTE BAG  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 19 : \n",
      "('CHARLOTTE BAG PINK POLKADOT', 'CHARLOTTE BAG SUKI DESIGN') -> RED RETROSPOT CHARLOTTE BAG  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 20 : \n",
      "('JUMBO BAG PINK POLKADOT', 'JUMBO BAG SCANDINAVIAN BLUE PAISLEY') -> JUMBO BAG RED RETROSPOT  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 21 : \n",
      "('JUMBO SHOPPER VINTAGE RED PAISLEY', 'JUMBO BAG PINK POLKADOT') -> JUMBO BAG RED RETROSPOT  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 22 : \n",
      "('DOTCOM POSTAGE', 'JUMBO BAG PINK POLKADOT') -> JUMBO BAG RED RETROSPOT  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 23 : \n",
      "('JUMBO STORAGE BAG SUKI', 'JUMBO BAG PINK POLKADOT') -> JUMBO BAG RED RETROSPOT  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n",
      "Rule 24 : \n",
      "('JUMBO BAG STRAWBERRY', 'JUMBO BAG PINK POLKADOT') -> JUMBO BAG RED RETROSPOT  - interrest:  0.64806  - confidence:  0.80969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(rules_named), \"Rules Extracted: (Rules sorted by interest)\")\n",
    "print()\n",
    "index_format = \"%\" + str(math.ceil(math.log10(len(rules)))) + \"d\" # to make sure spacing will be the same for all rules\n",
    "for i in range(len(rules_named)):\n",
    "    print(\"Rule\", index_format%(i + 1), \": \")\n",
    "    print(rules_named[i][0][0], \"->\", rules_named[i][0][1], \n",
    "          \" - interrest: \", \"%1.5f\"%rules_named[i][2], \" - confidence: \", \"%1.5f\"%rules_named[i][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6234b5-27e5-4d38-8287-c45bf8942dd6",
   "metadata": {},
   "source": [
    "## Approach 2 - Only Using Spark (For 1st and 2nd Passes)\n",
    "Only First and Second Passes are implemented\n",
    "\n",
    "This Approach is slower and requires more memory\n",
    "\n",
    "1. Finding Frequent Items and Frequent Buckets (Count Using Map (flatMap) and Reduce (By Key) and then filtering)\n",
    "2. Creating All pairs of Frequent Items, Filtering out pairs with same item in both places, map them to their hash (hash, pair), joining them with frequent buckets to filter out pairs that are in an infrequent bucket)\n",
    "3. Creating All combinations (of two items) for each basket, joining them with result of previous step (to eliminate unnecessary pairs)\n",
    "4. Counting remaining pairs using MapReduce and Filtering Pairs with support above set threshold\n",
    "5. Next passes\n",
    "    1. Creating sets from frequent sets of last step (sets of size n+1 from sets of size n)\n",
    "    2. Creating sets using union, if a set has exactly n+1 items we check if it has n+1 subsets in frequent sets of last step (to minimize candidate sets, all subsets of a frequent set must be frequent as well)\n",
    "    3. Map: if all elements of a candidate sets are present in a basket, we map it to (set, 1)\n",
    "    4. Counting using reduce by key, then filtering\n",
    "    5. Continuing to next pass (repeat this cycle) as long as this pass yields atleast n+1 frequent set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb4789c7-c28c-4dff-9333-870413ec9ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:============================================>           (79 + 8) / 99]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Frequent Pairs:  389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "frequent_items = baskets.flatMap(\n",
    "    lambda x: [(item, 1) for item in x[1]]\n",
    ").reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > support).keys()\n",
    "\n",
    "frequent_buckets = baskets.flatMap(\n",
    "    lambda x: [(hash_function(comb), 1) for comb in combinations(x[1], 2)]\n",
    ").reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > support).keys()\n",
    "\n",
    "# Step 2 + Transforming Data format for operations, the result will be frequent pairs\n",
    "s2 = frequent_items.cartesian(frequent_items).filter(lambda x: x[0] != x[1]).map(\n",
    "    lambda x: (hash_function(x), x)\n",
    ").join(frequent_buckets.map(lambda x: (x, None))).map(lambda x: x[1])\n",
    "\n",
    "# Step 3\n",
    "s3 = baskets.flatMap(lambda x: [(comb, 1) for comb in combinations(x[1], 2)]).join(s2)\n",
    "\n",
    "# Step 4\n",
    "frequent_pairs = s3.map(lambda x: (x[0], x[1][0])).reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > support)\n",
    "print(\"Number of Frequent Pairs: \", frequent_pairs.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
